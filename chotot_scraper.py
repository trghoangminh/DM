"""
Chotot.com Real Estate Scraper for Hanoi
Dá»± Ã¡n Data Mining - Báº¥t Ä‘á»™ng sáº£n HÃ  Ná»™i

Má»¥c Ä‘Ã­ch: Thu tháº­p dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n tá»« chotot.com khu vá»±c HÃ  Ná»™i
PhÆ°Æ¡ng phÃ¡p: Playwright (xá»­ lÃ½ JavaScript) + BeautifulSoup (parse HTML)
"""

import csv
import time
import random
from datetime import datetime, timedelta
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import re
import json

class ChoTotScraper:
    def __init__(self):
        # Chotot.com redirect sang nhatot.com cho báº¥t Ä‘á»™ng sáº£n
        self.base_url = "https://www.nhatot.com"
        self.hanoi_url = "https://www.nhatot.com/mua-ban-bat-dong-san-ha-noi"
        self.data = []
        
    def random_delay(self, min_seconds=2, max_seconds=5):
        """Táº¡o delay ngáº«u nhiÃªn giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh bá»‹ block"""
        delay = random.uniform(min_seconds, max_seconds)
        print(f"â³ Äá»£i {delay:.2f} giÃ¢y...")
        time.sleep(delay)
    
    def extract_price(self, text):
        """TrÃ­ch xuáº¥t giÃ¡ tá»« text"""
        if not text:
            return None
        # Xá»­ lÃ½ cÃ¡c format: "5 tá»·", "500 triá»‡u", "5.5 tá»·"
        text = text.lower().strip()
        if 'tá»·' in text:
            number = re.findall(r'[\d.,]+', text)
            if number:
                return f"{number[0]} tá»·"
        elif 'triá»‡u' in text:
            number = re.findall(r'[\d.,]+', text)
            if number:
                return f"{number[0]} triá»‡u"
        elif 'thá»a thuáº­n' in text or 'thoa thuan' in text:
            return "Thá»a thuáº­n"
        return text
    
    def extract_area(self, text):
        """TrÃ­ch xuáº¥t diá»‡n tÃ­ch tá»« text"""
        if not text:
            return None
        # Format: "50 mÂ²", "50m2"
        match = re.search(r'([\d.,]+)\s*m[Â²2]', text.lower())
        if match:
            return f"{match.group(1)} mÂ²"
        return text
    
    def parse_listing_page(self, html_content):
        """Parse trang danh sÃ¡ch Ä‘á»ƒ láº¥y links cÃ¡c bÃ i Ä‘Äƒng"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Nhatot.com sá»­ dá»¥ng links káº¿t thÃºc báº±ng .htm cho detail pages
        # Pattern: /mua-ban-nha-dat-{district}/{id}.htm
        links = []
        
        # TÃ¬m táº¥t cáº£ links cÃ³ .htm (Ä‘Ã¢y lÃ  detail pages)
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link['href']
            
            # Chá»‰ láº¥y links káº¿t thÃºc báº±ng .htm (detail pages)
            if href.endswith('.htm'):
                # Náº¿u lÃ  relative URL, thÃªm base_url
                if href.startswith('/'):
                    full_url = self.base_url + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                # Chá»‰ láº¥y links báº¥t Ä‘á»™ng sáº£n HÃ  Ná»™i
                if 'ha-noi' in full_url.lower() and full_url not in links:
                    links.append(full_url)
        
        print(f"âœ… TÃ¬m tháº¥y {len(links)} bÃ i Ä‘Äƒng trÃªn trang")
        return links
    
    def parse_detail_page(self, html_content, url):
        """Parse trang chi tiáº¿t Ä‘á»ƒ láº¥y thÃ´ng tin báº¥t Ä‘á»™ng sáº£n"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        property_data = {
            'url': url,
            'price': None,
            'area': None,
            'address': None,
            'district': None,
            'bedrooms': None,
            'bathrooms': None,
            'property_type': None,
            'posted_date': None,
            'description': None
        }
        
        # Thá»­ tÃ¬m JSON-LD data (nhiá»u website dÃ¹ng structured data)
        json_ld = soup.find('script', type='application/ld+json')
        if json_ld:
            try:
                data = json.loads(json_ld.string)
                if isinstance(data, dict):
                    property_data['price'] = data.get('offers', {}).get('price')
                    property_data['address'] = data.get('address', {}).get('streetAddress')
            except:
                pass
        
        # TÃ¬m giÃ¡
        price_selectors = [
            {'class': re.compile(r'.*price.*', re.I)},
            {'class': re.compile(r'.*gia.*', re.I)},
            '[itemprop="price"]',
        ]
        for selector in price_selectors:
            if isinstance(selector, dict):
                elem = soup.find(attrs=selector)
            else:
                elem = soup.select_one(selector)
            if elem:
                property_data['price'] = self.extract_price(elem.get_text(strip=True))
                break
        
        # TÃ¬m diá»‡n tÃ­ch
        area_selectors = [
            {'class': re.compile(r'.*area.*', re.I)},
            {'class': re.compile(r'.*dien.*tich.*', re.I)},
        ]
        for selector in area_selectors:
            elem = soup.find(attrs=selector)
            if elem:
                property_data['area'] = self.extract_area(elem.get_text(strip=True))
                break
        
        # TÃ¬m Ä‘á»‹a chá»‰
        address_selectors = [
            {'class': re.compile(r'.*address.*', re.I)},
            {'class': re.compile(r'.*dia.*chi.*', re.I)},
            '[itemprop="address"]',
        ]
        for selector in address_selectors:
            if isinstance(selector, dict):
                elem = soup.find(attrs=selector)
            else:
                elem = soup.select_one(selector)
            if elem:
                property_data['address'] = elem.get_text(strip=True)
                # TrÃ­ch xuáº¥t quáº­n/huyá»‡n tá»« Ä‘á»‹a chá»‰
                address_text = property_data['address']
                district_match = re.search(r'(Quáº­n|Huyá»‡n)\s+([^,]+)', address_text)
                if district_match:
                    property_data['district'] = f"{district_match.group(1)} {district_match.group(2).strip()}"
                break
        
        # TÃ¬m cÃ¡c thÃ´ng tin khÃ¡c trong báº£ng thÃ´ng sá»‘
        # Chotot thÆ°á»ng cÃ³ báº£ng thÃ´ng sá»‘ ká»¹ thuáº­t
        spec_rows = soup.find_all(['tr', 'div'], class_=re.compile(r'.*(spec|attribute|param).*', re.I))
        
        for row in spec_rows:
            text = row.get_text().lower()
            
            # Sá»‘ phÃ²ng ngá»§
            if 'phÃ²ng ngá»§' in text or 'bedroom' in text:
                numbers = re.findall(r'\d+', text)
                if numbers:
                    property_data['bedrooms'] = numbers[0]
            
            # Sá»‘ phÃ²ng táº¯m
            if 'phÃ²ng táº¯m' in text or 'toilet' in text or 'bathroom' in text:
                numbers = re.findall(r'\d+', text)
                if numbers:
                    property_data['bathrooms'] = numbers[0]
            
            # Loáº¡i hÃ¬nh
            if 'loáº¡i hÃ¬nh' in text or 'property type' in text or 'loáº¡i bds' in text:
                property_data['property_type'] = row.get_text(strip=True).split(':')[-1].strip()
        
        # TÃ¬m ngÃ y Ä‘Äƒng
        date_selectors = [
            {'class': re.compile(r'.*date.*', re.I)},
            {'class': re.compile(r'.*time.*', re.I)},
            '[itemprop="datePublished"]',
        ]
        for selector in date_selectors:
            if isinstance(selector, dict):
                elem = soup.find(attrs=selector)
            else:
                elem = soup.select_one(selector)
            if elem:
                date_text = elem.get_text(strip=True)
                property_data['posted_date'] = date_text
                break
        
        # TÃ¬m mÃ´ táº£
        desc_selectors = [
            {'class': re.compile(r'.*description.*', re.I)},
            {'class': re.compile(r'.*mo.*ta.*', re.I)},
            '[itemprop="description"]',
        ]
        for selector in desc_selectors:
            if isinstance(selector, dict):
                elem = soup.find(attrs=selector)
            else:
                elem = soup.select_one(selector)
            if elem:
                property_data['description'] = elem.get_text(strip=True)[:500]  # Giá»›i háº¡n 500 kÃ½ tá»±
                break
        
        return property_data
    
    def scrape(self, max_pages=5, max_items_per_page=20):
        """
        HÃ m chÃ­nh Ä‘á»ƒ crawl dá»¯ liá»‡u
        
        Args:
            max_pages: Sá»‘ trang tá»‘i Ä‘a cáº§n crawl
            max_items_per_page: Sá»‘ bÃ i Ä‘Äƒng tá»‘i Ä‘a má»—i trang
        """
        print("ğŸš€ Báº¯t Ä‘áº§u crawl dá»¯ liá»‡u tá»« Chotot.com")
        print(f"ğŸ“ Khu vá»±c: HÃ  Ná»™i")
        print(f"ğŸ“„ Sá»‘ trang tá»‘i Ä‘a: {max_pages}")
        print("-" * 60)
        
        with sync_playwright() as p:
            # Khá»Ÿi táº¡o browser vá»›i options Ä‘á»ƒ trÃ¡nh bá»‹ phÃ¡t hiá»‡n
            browser = p.chromium.launch(
                headless=False,  # Äáº·t True Ä‘á»ƒ cháº¡y ngáº§m, False Ä‘á»ƒ xem quÃ¡ trÃ¬nh
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                ]
            )
            
            # Táº¡o context vá»›i user agent giá»‘ng ngÆ°á»i dÃ¹ng tháº­t
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='vi-VN',
            )
            
            page = context.new_page()
            
            # Crawl tá»«ng trang danh sÃ¡ch
            for page_num in range(1, max_pages + 1):
                print(f"\nğŸ“„ Äang crawl trang {page_num}/{max_pages}")
                
                # URL cÃ³ thá»ƒ cÃ³ pagination: ?page=1, ?page=2, etc.
                if page_num == 1:
                    url = self.hanoi_url
                else:
                    url = f"{self.hanoi_url}?page={page_num}"
                
                try:
                    # Truy cáº­p trang danh sÃ¡ch
                    print(f"ğŸŒ Äang truy cáº­p: {url}")
                    page.goto(url, wait_until='networkidle', timeout=30000)
                    
                    # Scroll Ä‘á»ƒ load lazy content
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(2)
                    
                    # Láº¥y HTML content
                    html_content = page.content()
                    
                    # Parse Ä‘á»ƒ láº¥y links
                    listing_links = self.parse_listing_page(html_content)
                    
                    if not listing_links:
                        print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y bÃ i Ä‘Äƒng nÃ o, cÃ³ thá»ƒ Ä‘Ã£ háº¿t trang hoáº·c cáº§n cáº­p nháº­t selector")
                        break
                    
                    # Giá»›i háº¡n sá»‘ bÃ i Ä‘Äƒng má»—i trang
                    listing_links = listing_links[:max_items_per_page]
                    
                    # Crawl tá»«ng bÃ i Ä‘Äƒng chi tiáº¿t
                    for idx, detail_url in enumerate(listing_links, 1):
                        print(f"\n  ğŸ“Œ [{idx}/{len(listing_links)}] Äang crawl: {detail_url}")
                        
                        try:
                            page.goto(detail_url, wait_until='networkidle', timeout=30000)
                            time.sleep(1)
                            
                            # Scroll Ä‘á»ƒ load háº¿t ná»™i dung
                            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                            time.sleep(1)
                            
                            detail_html = page.content()
                            property_data = self.parse_detail_page(detail_html, detail_url)
                            
                            self.data.append(property_data)
                            print(f"  âœ… ÄÃ£ láº¥y dá»¯ liá»‡u: {property_data['price']} - {property_data['area']}")
                            
                        except Exception as e:
                            print(f"  âŒ Lá»—i khi crawl chi tiáº¿t: {e}")
                        
                        # Delay giá»¯a cÃ¡c request
                        self.random_delay(2, 4)
                    
                except Exception as e:
                    print(f"âŒ Lá»—i khi crawl trang {page_num}: {e}")
                
                # Delay giá»¯a cÃ¡c trang
                self.random_delay(3, 6)
            
            browser.close()
        
        print(f"\n{'='*60}")
        print(f"âœ… HoÃ n thÃ nh! ÄÃ£ crawl Ä‘Æ°á»£c {len(self.data)} bÃ i Ä‘Äƒng")
        print(f"{'='*60}")
    
    def save_to_csv(self, filename='chotot_hanoi_data.csv'):
        """LÆ°u dá»¯ liá»‡u ra file CSV"""
        if not self.data:
            print("âš ï¸  KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u")
            return
        
        fieldnames = ['url', 'price', 'area', 'address', 'district', 'bedrooms', 
                     'bathrooms', 'property_type', 'posted_date', 'description']
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.data)
        
        print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(self.data)} báº£n ghi vÃ o file: {filename}")
        print(f"ğŸ“Š CÃ¡c cá»™t: {', '.join(fieldnames)}")


def main():
    """HÃ m main Ä‘á»ƒ cháº¡y scraper"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     CHOTOT.COM SCRAPER - Dá»° ÃN DATA MINING HÃ€ Ná»˜I      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    scraper = ChoTotScraper()
    
    # Cáº¥u hÃ¬nh crawl
    MAX_PAGES = 3          # Sá»‘ trang cáº§n crawl (báº¯t Ä‘áº§u vá»›i 3 trang Ä‘á»ƒ test)
    MAX_ITEMS_PER_PAGE = 10  # Sá»‘ bÃ i Ä‘Äƒng má»—i trang (báº¯t Ä‘áº§u vá»›i 10 Ä‘á»ƒ test)
    
    print(f"âš™ï¸  Cáº¥u hÃ¬nh:")
    print(f"   - Sá»‘ trang: {MAX_PAGES}")
    print(f"   - Sá»‘ bÃ i Ä‘Äƒng/trang: {MAX_ITEMS_PER_PAGE}")
    print(f"   - Tá»•ng dá»± kiáº¿n: ~{MAX_PAGES * MAX_ITEMS_PER_PAGE} bÃ i Ä‘Äƒng")
    print()
    
    # Báº¯t Ä‘áº§u crawl
    scraper.scrape(max_pages=MAX_PAGES, max_items_per_page=MAX_ITEMS_PER_PAGE)
    
    # LÆ°u dá»¯ liá»‡u
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"chotot_hanoi_{timestamp}.csv"
    scraper.save_to_csv(filename)
    
    print(f"\nâœ¨ HoÃ n táº¥t! Kiá»ƒm tra file: {filename}")


if __name__ == "__main__":
    main()
