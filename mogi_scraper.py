"""
Mogi.vn Real Estate Scraper for Hanoi
Dá»± Ã¡n Data Mining - Báº¥t Ä‘á»™ng sáº£n HÃ  Ná»™i

Má»¥c Ä‘Ã­ch: Thu tháº­p dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n tá»« mogi.vn khu vá»±c HÃ  Ná»™i
PhÆ°Æ¡ng phÃ¡p: Playwright + BeautifulSoup
Æ¯u Ä‘iá»ƒm: Mogi.vn KHÃ”NG cÃ³ Cloudflare protection!
"""

import csv
import time
import random
from datetime import datetime
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import re

class MogiScraper:
    def __init__(self):
        self.base_url = "https://mogi.vn"
        self.hanoi_url = "https://mogi.vn/ha-noi/mua-mat-bang-cua-hang-shop"  # Máº·t báº±ng HÃ  Ná»™i
        self.data = []
        
    def random_delay(self, min_seconds=2, max_seconds=4):
        """Táº¡o delay ngáº«u nhiÃªn giá»¯a cÃ¡c request"""
        delay = random.uniform(min_seconds, max_seconds)
        print(f"â³ Äá»£i {delay:.2f} giÃ¢y...")
        time.sleep(delay)
    
    def clean_text(self, text):
        """LÃ m sáº¡ch text"""
        if not text:
            return None
        return ' '.join(text.strip().split())
    
    def extract_price(self, price_text):
        """TrÃ­ch xuáº¥t giÃ¡"""
        if not price_text:
            return None
        price_text = self.clean_text(price_text)
        # Giá»¯ nguyÃªn format: "5 tá»·", "500 triá»‡u"
        return price_text
    
    def extract_area(self, area_text):
        """TrÃ­ch xuáº¥t diá»‡n tÃ­ch"""
        if not area_text:
            return None
        # Format: "50 mÂ²"
        match = re.search(r'([\d.,]+)\s*m[Â²2]', area_text)
        if match:
            return f"{match.group(1)} mÂ²"
        return self.clean_text(area_text)
    
    def parse_listing_page(self, html_content):
        """Parse trang danh sÃ¡ch Ä‘á»ƒ láº¥y links cÃ¡c bÃ i Ä‘Äƒng"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        links = []
        
        # Mogi.vn sá»­ dá»¥ng class 'link-overlay' cho links chi tiáº¿t
        link_elements = soup.select('a.link-overlay')
        
        for elem in link_elements:
            href = elem.get('href', '')
            if href:
                # Náº¿u lÃ  relative URL, thÃªm base_url
                if href.startswith('/'):
                    full_url = self.base_url + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                # CHá»ˆ Láº¤Y URLs cÃ³ pattern listing tháº­t: /quan-*/mua-*/...-id[sá»‘]
                # Loáº¡i bá»: /gia-nha-dat, /10-buoc-mua-nha, etc.
                if re.search(r'-id\d+$', full_url) and full_url not in links:
                    links.append(full_url)
        
        print(f"âœ… TÃ¬m tháº¥y {len(links)} bÃ i Ä‘Äƒng há»£p lá»‡ trÃªn trang")
        return links
    
    def parse_detail_page(self, html_content, url):
        """Parse trang chi tiáº¿t Ä‘á»ƒ láº¥y thÃ´ng tin báº¥t Ä‘á»™ng sáº£n"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        property_data = {
            'url': url,
            'price': None,
            'area': None,
            'address': None,
            'district': None,
            'bedrooms': None,
            'bathrooms': None,
            'property_type': None,
            'posted_date': None,
            'description': None
        }
        
        # GiÃ¡
        price_elem = soup.select_one('.price')
        if price_elem:
            property_data['price'] = self.extract_price(price_elem.get_text())
        
        # Äá»‹a chá»‰
        address_elem = soup.select_one('.address')
        if address_elem:
            address_text = self.clean_text(address_elem.get_text())
            property_data['address'] = address_text
            
            # TrÃ­ch xuáº¥t quáº­n tá»« Ä‘á»‹a chá»‰
            # Format: "ÄÆ°á»ng ABC, PhÆ°á»ng XYZ, Quáº­n HoÃ n Kiáº¿m, HÃ  Ná»™i"
            parts = address_text.split(',')
            for part in parts:
                part = part.strip()
                if 'quáº­n' in part.lower() or 'huyá»‡n' in part.lower():
                    property_data['district'] = part
                    break
        
        # CÃ¡c thÃ´ng tin tá»« info-attr
        info_attrs = soup.select('.info-attr')
        for attr in info_attrs:
            spans = attr.find_all('span')
            if len(spans) >= 2:
                label = self.clean_text(spans[0].get_text()).lower()
                value = self.clean_text(spans[-1].get_text())
                
                if 'diá»‡n tÃ­ch' in label:
                    property_data['area'] = self.extract_area(value)
                elif 'phÃ²ng ngá»§' in label:
                    property_data['bedrooms'] = value
                elif 'nhÃ  táº¯m' in label or 'toilet' in label:
                    property_data['bathrooms'] = value
                elif 'ngÃ y Ä‘Äƒng' in label:
                    property_data['posted_date'] = value
        
        # Loáº¡i hÃ¬nh tá»« breadcrumb
        breadcrumbs = soup.select('.breadcrumb li a')
        if breadcrumbs:
            # Láº¥y pháº§n tá»­ cuá»‘i cÃ¹ng
            property_data['property_type'] = self.clean_text(breadcrumbs[-1].get_text())
        
        # MÃ´ táº£
        # Thá»­ nhiá»u selector
        desc_selectors = ['.introduction', '.property-description', '.info-content-body']
        for selector in desc_selectors:
            desc_elem = soup.select_one(selector)
            if desc_elem:
                desc_text = self.clean_text(desc_elem.get_text())
                property_data['description'] = desc_text[:500]  # Giá»›i háº¡n 500 kÃ½ tá»±
                break
        
        return property_data
    
    def scrape(self, max_pages=3, max_items_per_page=10, auto_save=True):
        """
        HÃ m chÃ­nh Ä‘á»ƒ crawl dá»¯ liá»‡u
        
        Args:
            max_pages: Sá»‘ trang tá»‘i Ä‘a cáº§n crawl
            max_items_per_page: Sá»‘ bÃ i Ä‘Äƒng tá»‘i Ä‘a má»—i trang
            auto_save: Tá»± Ä‘á»™ng lÆ°u sau má»—i trang (trÃ¡nh máº¥t dá»¯ liá»‡u)
        """
        print("ğŸš€ Báº¯t Ä‘áº§u crawl dá»¯ liá»‡u tá»« Mogi.vn")
        print(f"ğŸ“ Khu vá»±c: HÃ  Ná»™i")
        print(f"ğŸ“„ Sá»‘ trang tá»‘i Ä‘a: {max_pages}")
        print(f"ğŸ’¾ Auto-save: {'Báº­t' if auto_save else 'Táº¯t'}")
        print("-" * 60)
        
        with sync_playwright() as p:
            # Khá»Ÿi táº¡o browser
            browser = p.chromium.launch(
                headless=False,  # Äáº·t True Ä‘á»ƒ cháº¡y ngáº§m
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='vi-VN',
            )
            
            page = context.new_page()
            
            # Crawl tá»«ng trang danh sÃ¡ch
            for page_num in range(1, max_pages + 1):
                print(f"\nğŸ“„ Äang crawl trang {page_num}/{max_pages}")
                
                # URL pagination: /ha-noi/mua-nha-dat?page=2
                if page_num == 1:
                    url = self.hanoi_url
                else:
                    url = f"{self.hanoi_url}?page={page_num}"
                
                try:
                    print(f"ğŸŒ Äang truy cáº­p: {url}")
                    page.goto(url, wait_until='domcontentloaded', timeout=30000)
                    
                    # Äá»£i listings load
                    time.sleep(2)
                    
                    # Scroll Ä‘á»ƒ load lazy content
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(1)
                    
                    # Láº¥y HTML content
                    html_content = page.content()
                    
                    # Parse Ä‘á»ƒ láº¥y links
                    listing_links = self.parse_listing_page(html_content)
                    
                    if not listing_links:
                        print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y bÃ i Ä‘Äƒng nÃ o")
                        break
                    
                    # Giá»›i háº¡n sá»‘ bÃ i Ä‘Äƒng má»—i trang
                    listing_links = listing_links[:max_items_per_page]
                    
                    # Crawl tá»«ng bÃ i Ä‘Äƒng chi tiáº¿t
                    for idx, detail_url in enumerate(listing_links, 1):
                        print(f"\n  ğŸ“Œ [{idx}/{len(listing_links)}] Äang crawl: {detail_url}")
                        
                        try:
                            page.goto(detail_url, wait_until='domcontentloaded', timeout=30000)
                            time.sleep(1)
                            
                            # Scroll Ä‘á»ƒ load háº¿t ná»™i dung
                            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                            time.sleep(1)
                            
                            detail_html = page.content()
                            property_data = self.parse_detail_page(detail_html, detail_url)
                            
                            self.data.append(property_data)
                            print(f"  âœ… ÄÃ£ láº¥y dá»¯ liá»‡u: {property_data['price']} - {property_data['area']}")
                            
                        except Exception as e:
                            print(f"  âŒ Lá»—i khi crawl chi tiáº¿t: {e}")
                        
                        # Delay giá»¯a cÃ¡c request
                        self.random_delay(2, 3)
                    
                    # Auto-save sau má»—i trang (trÃ¡nh máº¥t dá»¯ liá»‡u khi dá»«ng giá»¯a chá»«ng)
                    if auto_save and self.data:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        temp_filename = f"mogi_hanoi_{timestamp}_temp.csv"
                        self.save_to_csv(temp_filename)
                        print(f"  ğŸ’¾ ÄÃ£ lÆ°u táº¡m {len(self.data)} bÃ i vÃ o: {temp_filename}")
                    
                except Exception as e:
                    print(f"âŒ Lá»—i khi crawl trang {page_num}: {e}")
                
                # Delay giá»¯a cÃ¡c trang
                self.random_delay(3, 5)
            
            browser.close()
        
        print(f"\n{'='*60}")
        print(f"âœ… HoÃ n thÃ nh! ÄÃ£ crawl Ä‘Æ°á»£c {len(self.data)} bÃ i Ä‘Äƒng")
        print(f"{'='*60}")
    
    def save_to_csv(self, filename='mogi_hanoi_data.csv'):
        """LÆ°u dá»¯ liá»‡u ra file CSV"""
        if not self.data:
            print("âš ï¸  KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u")
            return
        
        fieldnames = ['url', 'price', 'area', 'address', 'district', 'bedrooms', 
                     'bathrooms', 'property_type', 'posted_date', 'description']
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.data)
        
        print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(self.data)} báº£n ghi vÃ o file: {filename}")
        print(f"ğŸ“Š CÃ¡c cá»™t: {', '.join(fieldnames)}")


def main():
    """HÃ m main Ä‘á»ƒ cháº¡y scraper"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘       MOGI.VN SCRAPER - Dá»° ÃN DATA MINING HÃ€ Ná»˜I       â•‘
    â•‘              âœ… KHÃ”NG CÃ“ CLOUDFLARE BLOCK!              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    scraper = MogiScraper()
    
    # Cáº¥u hÃ¬nh crawl - Máº·t báº±ng/cá»­a hÃ ng Ã­t trÃ¹ng láº·p hÆ¡n
    MAX_PAGES = 30         # 30 trang cho máº·t báº±ng
    MAX_ITEMS_PER_PAGE = 20  # 20 bÃ i Ä‘Äƒng má»—i trang
    # Dá»± kiáº¿n: ~600 bÃ i â†’ sau loáº¡i trÃ¹ng: ~150-300 bÃ i duy nháº¥t (Ã­t trÃ¹ng hÆ¡n)
    
    print(f"âš™ï¸  Cáº¥u hÃ¬nh:")
    print(f"   - Sá»‘ trang: {MAX_PAGES}")
    print(f"   - Sá»‘ bÃ i Ä‘Äƒng/trang: {MAX_ITEMS_PER_PAGE}")
    print(f"   - Tá»•ng dá»± kiáº¿n: ~{MAX_PAGES * MAX_ITEMS_PER_PAGE} bÃ i Ä‘Äƒng")
    print()
    
    # Báº¯t Ä‘áº§u crawl
    scraper.scrape(max_pages=MAX_PAGES, max_items_per_page=MAX_ITEMS_PER_PAGE)
    
    # LÆ°u dá»¯ liá»‡u
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"mogi_hanoi_{timestamp}.csv"
    scraper.save_to_csv(filename)
    
    print(f"\nâœ¨ HoÃ n táº¥t! Kiá»ƒm tra file: {filename}")
    print(f"\nğŸ“Š Äá»ƒ phÃ¢n tÃ­ch dá»¯ liá»‡u, cháº¡y:")
    print(f"   python3 analyze_data.py")


if __name__ == "__main__":
    main()
