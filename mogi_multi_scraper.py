#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Mogi.vn Multi-Category Scraper - TrÃ¡nh trÃ¹ng láº·p báº±ng cÃ¡ch crawl nhiá»u danh má»¥c
"""

import time
import random
import re
from datetime import datetime
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import csv

class MogiMultiCategoryScraper:
    def __init__(self):
        self.base_url = "https://mogi.vn"
        
        # CHIáº¾N LÆ¯á»¢C: Crawl nhiá»u loáº¡i hÃ¬nh BÄS khÃ¡c nhau
        self.categories = [
            "/ha-noi/mua-can-ho",                    # CÄƒn há»™
            "/ha-noi/mua-nha-rieng",                 # NhÃ  riÃªng
            "/ha-noi/mua-nha-mat-tien-pho",          # NhÃ  máº·t phá»‘
            "/ha-noi/mua-nha-biet-thu-lien-ke",      # Biá»‡t thá»±
            "/ha-noi/mua-dat-nen-du-an",             # Äáº¥t ná»n
            "/ha-noi/mua-mat-bang-cua-hang-shop",    # Máº·t báº±ng
        ]
        
        self.data = []
        self.seen_urls = set()  # Track URLs Ä‘Ã£ crawl Ä‘á»ƒ trÃ¡nh trÃ¹ng
        
    def random_delay(self, min_seconds=2, max_seconds=4):
        delay = random.uniform(min_seconds, max_seconds)
        print(f"â³ Äá»£i {delay:.2f} giÃ¢y...")
        time.sleep(delay)
    
    def clean_text(self, text):
        if not text:
            return None
        return ' '.join(text.split()).strip()
    
    def extract_price(self, price_text):
        if not price_text:
            return None
        price_text = self.clean_text(price_text)
        return price_text
    
    def extract_area(self, area_text):
        if not area_text:
            return None
        area_text = self.clean_text(area_text)
        return area_text
    
    def parse_listing_page(self, html_content):
        soup = BeautifulSoup(html_content, 'html.parser')
        links = []
        link_elements = soup.select('a.link-overlay')
        
        for elem in link_elements:
            href = elem.get('href', '')
            if href:
                if href.startswith('/'):
                    full_url = self.base_url + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    continue
                
                # Chá»‰ láº¥y URLs há»£p lá»‡ vÃ  chÆ°a crawl
                if re.search(r'-id\d+$', full_url) and full_url not in self.seen_urls:
                    links.append(full_url)
                    self.seen_urls.add(full_url)  # ÄÃ¡nh dáº¥u Ä‘Ã£ tháº¥y
        
        return links
    
    def parse_detail_page(self, html_content, url):
        soup = BeautifulSoup(html_content, 'html.parser')
        
        property_data = {
            'url': url,
            'price': None,
            'area': None,
            'address': None,
            'district': None,
            'bedrooms': None,
            'bathrooms': None,
            'property_type': None,
            'posted_date': None,
            'description': None
        }
        
        # GiÃ¡
        price_elem = soup.select_one('.price')
        if price_elem:
            property_data['price'] = self.extract_price(price_elem.get_text())
        
        # Äá»‹a chá»‰
        address_elem = soup.select_one('.address')
        if address_elem:
            address_text = self.clean_text(address_elem.get_text())
            property_data['address'] = address_text
            
            # TrÃ­ch xuáº¥t quáº­n
            parts = address_text.split(',')
            for part in parts:
                part = part.strip()
                if 'quáº­n' in part.lower() or 'huyá»‡n' in part.lower():
                    property_data['district'] = part
                    break
        
        # ThÃ´ng tin tá»« info-attr
        info_attrs = soup.select('.info-attr')
        for attr in info_attrs:
            spans = attr.find_all('span')
            if len(spans) >= 2:
                label = self.clean_text(spans[0].get_text()).lower()
                value = self.clean_text(spans[-1].get_text())
                
                if 'diá»‡n tÃ­ch' in label:
                    property_data['area'] = self.extract_area(value)
                elif 'phÃ²ng ngá»§' in label:
                    property_data['bedrooms'] = value
                elif 'nhÃ  táº¯m' in label or 'toilet' in label:
                    property_data['bathrooms'] = value
                elif 'ngÃ y Ä‘Äƒng' in label:
                    property_data['posted_date'] = value
        
        # Loáº¡i hÃ¬nh
        breadcrumbs = soup.select('.breadcrumb li a')
        if breadcrumbs:
            property_data['property_type'] = self.clean_text(breadcrumbs[-1].get_text())
        
        # MÃ´ táº£
        desc_selectors = ['.introduction', '.property-description', '.info-content-body']
        for selector in desc_selectors:
            desc_elem = soup.select_one(selector)
            if desc_elem:
                desc_text = self.clean_text(desc_elem.get_text())
                property_data['description'] = desc_text[:500]
                break
        
        return property_data
    
    def scrape_category(self, category_url, max_pages=5, max_items_per_page=20):
        """Crawl má»™t danh má»¥c cá»¥ thá»ƒ"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‚ Äang crawl danh má»¥c: {category_url}")
        print(f"{'='*60}")
        
        category_data = []
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='vi-VN',
            )
            page = context.new_page()
            
            for page_num in range(1, max_pages + 1):
                print(f"\nğŸ“„ Trang {page_num}/{max_pages}")
                
                if page_num == 1:
                    url = self.base_url + category_url
                else:
                    url = f"{self.base_url}{category_url}?page={page_num}"
                
                try:
                    page.goto(url, wait_until='domcontentloaded', timeout=30000)
                    time.sleep(2)
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(1)
                    
                    html_content = page.content()
                    listing_links = self.parse_listing_page(html_content)
                    
                    if not listing_links:
                        print("âš ï¸  KhÃ´ng cÃ³ bÃ i má»›i")
                        break
                    
                    print(f"âœ… TÃ¬m tháº¥y {len(listing_links)} bÃ i Má»šI (chÆ°a crawl)")
                    
                    listing_links = listing_links[:max_items_per_page]
                    
                    for idx, detail_url in enumerate(listing_links, 1):
                        print(f"  ğŸ“Œ [{idx}/{len(listing_links)}] {detail_url}")
                        
                        try:
                            page.goto(detail_url, wait_until='domcontentloaded', timeout=30000)
                            time.sleep(1)
                            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                            time.sleep(1)
                            
                            detail_html = page.content()
                            property_data = self.parse_detail_page(detail_html, detail_url)
                            
                            category_data.append(property_data)
                            self.data.append(property_data)
                            print(f"  âœ… {property_data['price']} - {property_data['area']}")
                            
                        except Exception as e:
                            print(f"  âŒ Lá»—i: {e}")
                        
                        self.random_delay(2, 3)
                    
                except Exception as e:
                    print(f"âŒ Lá»—i trang {page_num}: {e}")
                
                self.random_delay(3, 5)
            
            browser.close()
        
        print(f"\nâœ… Danh má»¥c nÃ y: {len(category_data)} bÃ i")
        return category_data
    
    def save_to_csv(self, filename=None):
        if not self.data:
            print("âš ï¸  KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"mogi_hanoi_multicategory_{timestamp}.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['url', 'price', 'area', 'address', 'district', 
                         'bedrooms', 'bathrooms', 'property_type', 'posted_date', 'description']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.data)
        
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u {len(self.data)} báº£n ghi vÃ o: {filename}")
        return filename

def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘    MOGI.VN MULTI-CATEGORY SCRAPER - TRÃNH TRÃ™NG Láº¶P    â•‘
    â•‘         Crawl nhiá»u danh má»¥c Ä‘á»ƒ láº¥y dá»¯ liá»‡u Ä‘a dáº¡ng      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    scraper = MogiMultiCategoryScraper()
    
    # Cáº¤U HÃŒNH Tá»I ÄA - Láº¤Y NHIá»€U Dá»® LIá»†U NHáº¤T
    PAGES_PER_CATEGORY = 50  # 50 trang/danh má»¥c (tá»‘i Ä‘a)
    ITEMS_PER_PAGE = 20      # 20 bÃ i/trang
    
    print(f"âš™ï¸  Cáº¤U HÃŒNH Tá»I ÄA:")
    print(f"   - Sá»‘ danh má»¥c: {len(scraper.categories)}")
    print(f"   - Sá»‘ trang/danh má»¥c: {PAGES_PER_CATEGORY}")
    print(f"   - Sá»‘ bÃ i/trang: {ITEMS_PER_PAGE}")
    print(f"   - Dá»± kiáº¿n: ~{len(scraper.categories) * PAGES_PER_CATEGORY * 20} bÃ i")
    print(f"   - Ká»³ vá»ng sau loáº¡i trÃ¹ng: 500-1000 bÃ i duy nháº¥t")
    print(f"   - Thá»i gian Æ°á»›c tÃ­nh: 4-6 giá»")
    print(f"\nğŸ’¡ Khuyáº¿n nghá»‹: Cháº¡y qua Ä‘Ãªm!")
    
    # Crawl tá»«ng danh má»¥c
    for category in scraper.categories:
        scraper.scrape_category(category, max_pages=PAGES_PER_CATEGORY, max_items_per_page=ITEMS_PER_PAGE)
    
    # LÆ°u káº¿t quáº£
    filename = scraper.save_to_csv()
    
    print(f"\n{'='*60}")
    print(f"âœ… HOÃ€N THÃ€NH!")
    print(f"   - Tá»•ng sá»‘ bÃ i: {len(scraper.data)}")
    print(f"   - Sá»‘ URLs duy nháº¥t: {len(scraper.seen_urls)}")
    print(f"   - File: {filename}")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
